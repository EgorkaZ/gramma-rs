use super::{
    tokenizer::{Token::*, *},
    AutomataBuilder, SubNFA, StrEdge, RangeEdge, EpsEdge,
    StatePtr, State, Registry, GrammarError,
};
use lalrpop_util::ParseError;

grammar<'input>(nfa: &mut AutomataBuilder, reg: &mut Registry);

pub Lexer: StatePtr = {
    "lexer" <decls:LexDecl*> "lexer_end" =>? {
        let start = nfa.add_state(State::casual());

        let toks = decls.iter()
            .filter_map(|(mb_tok_name, sub)| mb_tok_name.map(|name: &'input str| (name, sub)));

        for (name, sub) in toks {
            nfa.add_sym(&start, EpsEdge, sub.input());

            reg.set_tok(name)
                .map(|id| nfa.add_state(State::terminal(id)))
                .map(|term: StatePtr| nfa.add_sym(sub.output(), EpsEdge, &term))
                .map_err(|err| ParseError::User{ error: GrammarError::Registry(err) })?;
        }
        Ok(start)
    }
}

LexDecl: (Option<&'input str>, SubNFA) = {
    <tok:("tok"?)> <id:Id> "=" <curr:LexSeq> ";" => {
        (tok.map(|_| id), nfa.define_name(id, SubNFA::clone(&curr)))
    }
}

LexSeq: SubNFA = {
    <seq:LexPrimQual+> => {
        let input = StatePtr::clone(seq[0].input());
        let mut prev_to = seq[0].output();
        for SubNFA(from, to) in seq[1..].iter() {
            prev_to.extend(from);
            prev_to = to;
        }
        SubNFA(input, StatePtr::clone(prev_to))
    },
}

LexAlt: SubNFA = {
    "{" <fst:LexSeq> <rest:("," <LexSeq>)*> "}" => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), EpsEdge, fst.input());
        nfa.add_sym(fst.output(), EpsEdge, curr.output());

        for sub in rest.iter() {
            nfa.add_sym(curr.input(), EpsEdge, sub.input());
            nfa.add_sym(sub.output(), EpsEdge, curr.output());
        }
        curr
    }
}

LexPrimQual: SubNFA = {
    LexPrim,
    <sub:LexPrim> "?" => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), EpsEdge, sub.input());
        nfa.add_sym(sub.output(), EpsEdge, curr.output());
        nfa.add_sym(curr.input(), EpsEdge, curr.output());
        curr
    },
    <sub:LexPrim> "+" => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), EpsEdge, sub.input());
        nfa.add_sym(sub.output(), EpsEdge, curr.output());
        nfa.add_sym(sub.output(), EpsEdge, sub.input());
        curr
    },
    <sub:LexPrim> "*" => {
        let curr = nfa.create_sub();
        // parse "+"
        nfa.add_sym(curr.input(), EpsEdge, sub.input());
        nfa.add_sym(sub.output(), EpsEdge, curr.output());
        nfa.add_sym(sub.output(), EpsEdge, sub.input());
        // and also eps
        nfa.add_sym(curr.input(), EpsEdge, curr.output());
        curr
    },
}

LexPrim: SubNFA = {
    Id => nfa.resolve_id(<>),
    LexAlt,
    Str => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), StrEdge(<>), curr.output());
        curr
    },
    <rng:Rng> => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), RangeEdge::new(rng.0, rng.1), curr.output());
        curr
    },
    "eps" => {
        let curr = nfa.create_sub();
        nfa.add_sym(curr.input(), EpsEdge, curr.output());
        curr
    }
}

extern {
    type Location = usize;
    type Error = GrammarError<'input>;

    enum Token<'input>
    {
        "lexer"       => LexerBegin,
        "lexer_end"   => LexerEnd,
        "tok"         => Tok,
        Id            => Id(<&'input str>),
        "{"           => LBrace,
        "}"           => RBrace,
        Rng           => Range(<char>, <char>),
        Str           => Str(<String>),
        "="           => Eq,
        "?"           => QuestionMark,
        "*"           => Star,
        "+"           => Plus,
        ","           => Comma,
        ";"           => Semicolon,
        "eps"         => Eps,

        "grammar"     => GrammarBegin,
        "grammar_end" => GrammarEnd,
        Action        => FatArrow(<&'input str>),
        ":"           => Colon,
    }
}
